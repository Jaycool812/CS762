{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = []\n",
    "with open(\"trg.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        content_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'B',\n",
       " 'the 4 202 353 bp genome of the alkaliphilic bacterium bacillus halodurans c-125 contains 4066 predicted protein coding sequences cdss 2141 527 of which have functional assignments 1182 29 of which are conserved cdss with unknown function and 743 18 3 of which have no match to any protein database among the total cdss 88 match sequences of proteins found only in bacillus subtilis and 667 are widely conserved in comparison with the proteins of various organisms including bsubtilis the b halodurans genome contains 112 transposase genes indicating that transposases have played an important evolutionary role in horizontal gene transfer and also in internal genetic rearrangement in the genome strain c-125 lacks some of the necessary genes for competence such as coms srfa and rapc supporting the fact that competence has not been demonstrated experimentally in c-125 there is no paralog of tupa encoding teichuronopeptide which contributes to alkaliphily in the c-125 genome and an ortholog of tupa cannot be found in the bsubtilis genome out of 11 sigma factors which belong to the extracytoplasmic function family 10 are unique to b halodurans suggesting that they may have a role in the special mechanism of adaptation to an alkaline environment']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract class label\n",
    "class_label = [x[1] for x in content_list[1:]]\n",
    "#Extract content\n",
    "content = [x[2].lower().split() for x in content_list[1:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):  #create wordlist\n",
    "    word_dict = {}\n",
    "    for document in dataSet:\n",
    "        for word in document:\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = 1\n",
    "            else:\n",
    "                word_dict[word] += 1\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def removePunctuation(value):\n",
    "    result = ''\n",
    "    for a in value:\n",
    "        if a not in string.punctuation:\n",
    "            result += a\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "stop_word = []\n",
    "for stopword in stopwords.words('english'):\n",
    "    stopword = removePunctuation(stopword)\n",
    "    stop_word.append(stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find out the most 1000 frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_occurring_words_1000 = [i[0] \n",
    "                             for i in sorted(createVocabList(content).items(), key = lambda kv:(kv[1], kv[0]),reverse=True) \n",
    "                             if i[0] not in stop_word][0:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1#multivariate NB\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1#multinomial NB\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = []\n",
    "for i in content:\n",
    "    train_matrix.append(setOfWords2Vec(most_occurring_words_1000,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix_extension = []\n",
    "for i in content:\n",
    "    train_matrix_extension.append(listOfWords2Vec(most_occurring_words_1000,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix = np.array(train_matrix)\n",
    "train_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 1, ..., 0, 0, 0],\n",
       "       [4, 1, 7, ..., 0, 0, 0],\n",
       "       [6, 3, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [2, 3, 0, ..., 0, 0, 0],\n",
       "       [4, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix_extension = np.array(train_matrix_extension)\n",
    "train_matrix_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B', 'A', 'E', ..., 'E', 'E', 'B'], dtype='<U1')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_label = np.array(class_label)\n",
    "class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '0', '1', ..., '0', '0', 'B'],\n",
       "       ['1', '1', '1', ..., '0', '0', 'A'],\n",
       "       ['1', '1', '1', ..., '0', '0', 'E'],\n",
       "       ...,\n",
       "       ['0', '1', '0', ..., '0', '0', 'E'],\n",
       "       ['1', '1', '0', ..., '0', '0', 'E'],\n",
       "       ['1', '1', '0', ..., '0', '0', 'B']], dtype='<U21')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_train_matrix = np.column_stack((train_matrix,class_label))\n",
    "combined_train_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['2', '0', '1', ..., '0', '0', 'B'],\n",
       "       ['4', '1', '7', ..., '0', '0', 'A'],\n",
       "       ['6', '3', '1', ..., '0', '0', 'E'],\n",
       "       ...,\n",
       "       ['0', '1', '0', ..., '0', '0', 'E'],\n",
       "       ['2', '3', '0', ..., '0', '0', 'E'],\n",
       "       ['4', '1', '0', ..., '0', '0', 'B']], dtype='<U21')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_train_matrix_extension = np.column_stack((train_matrix_extension,class_label))\n",
    "combined_train_matrix_extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB(train_matrix,class_label):\n",
    "    \n",
    "    train_matrix = train_matrix.astype(float)\n",
    "    num_of_Words = len(train_matrix[0])#1000\n",
    "\n",
    "    #apply laplace smoothing\n",
    "    pA_Num = np.ones(num_of_Words)\n",
    "    pB_Num = np.ones(num_of_Words)\n",
    "    pE_Num = np.ones(num_of_Words)\n",
    "    pV_Num = np.ones(num_of_Words)\n",
    "\n",
    "    pA_Denom = 1000\n",
    "    pB_Denom = 1000\n",
    "    pE_Denom = 1000\n",
    "    pV_Denom = 1000\n",
    "\n",
    "    for i in range(len(train_matrix)):\n",
    "        if class_label[i] == \"A\":\n",
    "            pA_Num += train_matrix[i]\n",
    "            pA_Denom += sum(train_matrix[i])\n",
    "        elif class_label[i] == \"B\":\n",
    "            pB_Num += train_matrix[i]\n",
    "            pB_Denom += sum(train_matrix[i])\n",
    "        elif class_label[i] == \"E\":\n",
    "            pE_Num += train_matrix[i]\n",
    "            pE_Denom += sum(train_matrix[i])\n",
    "        elif class_label[i] == \"V\":\n",
    "            pV_Num += train_matrix[i]\n",
    "            pV_Denom += sum(train_matrix[i])\n",
    "    \n",
    "    #logrithmically\n",
    "    pA_Vect = np.log(pA_Num / pA_Denom) #p(data|A) vector\n",
    "    pB_Vect = np.log(pB_Num / pB_Denom)\n",
    "    pE_Vect = np.log(pE_Num / pE_Denom)\n",
    "    pV_Vect = np.log(pV_Num / pV_Denom)\n",
    "    \n",
    "    return pA_Vect,pB_Vect,pE_Vect,pV_Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB_extension(train_matrix,class_label,boolean_matrix):\n",
    "    \n",
    "    train_matrix = train_matrix.astype(float)\n",
    "    num_of_Words = len(train_matrix[0]) #feature number\n",
    "    instance_length = len(train_matrix)\n",
    "\n",
    "    #apply laplace smoothing\n",
    "    pA_Num = np.ones(num_of_Words)\n",
    "    pB_Num = np.ones(num_of_Words)\n",
    "    pE_Num = np.ones(num_of_Words)\n",
    "    pV_Num = np.ones(num_of_Words)\n",
    "\n",
    "    pA_Denom = num_of_Words\n",
    "    pB_Denom = num_of_Words\n",
    "    pE_Denom = num_of_Words\n",
    "    pV_Denom = num_of_Words\n",
    "    \n",
    "    #inverse document frequency\n",
    "    count_list = np.zeros(num_of_Words)\n",
    "    for i in range(num_of_Words):\n",
    "        #count the total frequency of each word\n",
    "        count = np.sum(boolean_matrix[:,i])\n",
    "        count_list[i] = count\n",
    "        \n",
    "    temp_list = []\n",
    "    for i in range(num_of_Words):\n",
    "        #calculate the weight\n",
    "        column_result = np.log(len(train_matrix)/count_list[i]) * train_matrix[:,i]        \n",
    "        temp_list.append(column_result)\n",
    "        \n",
    "    train_matrix = np.array(temp_list).T\n",
    "\n",
    "    for i in range(len(train_matrix)):\n",
    "        if class_label[i] == \"A\":\n",
    "            pA_Num += train_matrix[i]\n",
    "            pA_Denom += sum(train_matrix[i])\n",
    "        elif class_label[i] == \"B\":\n",
    "            pB_Num += train_matrix[i]\n",
    "            pB_Denom += sum(train_matrix[i])\n",
    "        elif class_label[i] == \"E\":\n",
    "            pE_Num += train_matrix[i]\n",
    "            pE_Denom += sum(train_matrix[i])\n",
    "        elif class_label[i] == \"V\":\n",
    "            pV_Num += train_matrix[i]\n",
    "            pV_Denom += sum(train_matrix[i])\n",
    "    \n",
    "    #logrithmically\n",
    "    pA_Vect = np.log(pA_Num / pA_Denom) #p(data|A) vector\n",
    "    pB_Vect = np.log(pB_Num / pB_Denom)\n",
    "    pE_Vect = np.log(pE_Num / pE_Denom)\n",
    "    pV_Vect = np.log(pV_Num / pV_Denom)\n",
    "    \n",
    "    return pA_Vect,pB_Vect,pE_Vect,pV_Vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label_result(test_set,pa,pb,pe,pv,prior_prob_list):\n",
    "    test_set = test_set.astype(float)\n",
    "    result_pA = np.sum(test_set*pa,axis=1)+np.log(prior_prob_list[0])\n",
    "    result_pB = np.sum(test_set*pb,axis=1)+np.log(prior_prob_list[1])\n",
    "    result_pE = np.sum(test_set*pe,axis=1)+np.log(prior_prob_list[2])\n",
    "    result_pV = np.sum(test_set*pv,axis=1)+np.log(prior_prob_list[3])\n",
    "    test_label = []\n",
    "    for i in range(len(result_pA)):\n",
    "        max_value = max(result_pA[i],result_pB[i],result_pE[i],result_pV[i])\n",
    "        if max_value == result_pA[i]:\n",
    "            test_label.append(\"A\")\n",
    "        elif max_value == result_pB[i]:\n",
    "            test_label.append(\"B\")\n",
    "        elif max_value == result_pE[i]:\n",
    "            test_label.append(\"E\")\n",
    "        else:\n",
    "            test_label.append(\"V\")\n",
    "    return test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "\n",
    "def cross_validation(K,dataset):\n",
    "    np.random.seed(0)\n",
    "    shuffled_dataset = np.random.permutation(dataset)\n",
    "    testing = []\n",
    "    \n",
    "    training = []\n",
    "    each_proportion = int(len(train_matrix)/K)\n",
    "    for i in range(K):\n",
    "        testing.append(shuffled_dataset[each_proportion*i:each_proportion*(i+1)])\n",
    "        temp = []\n",
    "        for j in range(K):\n",
    "            \n",
    "            if i != j:\n",
    "                temp.append(shuffled_dataset[each_proportion*j:each_proportion*(j+1)])\n",
    "                \n",
    "        training.append(np.concatenate(temp))\n",
    "        \n",
    "    return testing,training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testing_set,training_set = cross_validation(10,combined_train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set_extension,training_set_extension = cross_validation(10,combined_train_matrix_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_score(K,training_set,testing_set,training_model):\n",
    "    print(\"Training model...\")\n",
    "    class_label_list = [\"A\",\"B\",\"E\",\"V\"]\n",
    "    prior_prob_list = []\n",
    "    score_list = []\n",
    "    for k in range(K):\n",
    "        train_k = training_set[k]\n",
    "        if signal == True:\n",
    "            pa,pb,pe,pv = training_model(train_k[:,:-1],train_k[:,-1],boolean_matrix[k][:,:-1].astype(float))\n",
    "        else:\n",
    "            pa,pb,pe,pv = training_model(train_k[:,:-1],train_k[:,-1])\n",
    "        for i in class_label_list:\n",
    "            p= len([a for a in train_k[:,-1] if a == i])/len(train_k[:,-1])\n",
    "            prior_prob_list.append(p)\n",
    "            \n",
    "        test_label = generate_label_result(testing_set[k][:,:-1],pa,pb,pe,pv,prior_prob_list)\n",
    "        \n",
    "        result1 = 0\n",
    "        for j in range(len(test_label)):\n",
    "            result1 += 1 if (list(test_label)[j] == list(testing_set[k][:,-1])[j]) else 0\n",
    "        score = float(result1/len(test_label))\n",
    "        score_list.append(score)\n",
    "    \n",
    "    \n",
    "    return f\"\"\"The mean accuarcy is of {K}-fold validation is {np.mean(score_list)}\"\"\"\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "signal = False\n",
    "Mean_score = cross_val_score(10,training_set,testing_set,trainNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The mean accuarcy is of 10-fold validation is 0.91325'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "signal = True\n",
    "boolean_matrix = training_set\n",
    "Mean_score_extension = cross_val_score(10,training_set_extension,testing_set,trainNB_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The mean accuarcy is of 10-fold validation is 0.92225'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mean_score_extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_list = [\"A\",\"B\",\"E\",\"V\"]\n",
    "prior_prob = []\n",
    "for i in class_label_list:\n",
    "    p= len([a for a in combined_train_matrix[:,-1] if a == i])/len(combined_train_matrix[:,-1])\n",
    "    prior_prob.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.032, 0.4005, 0.536, 0.0315]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_content = []\n",
    "with open(\"tst.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        test_content.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [x[1].lower().split() for x in test_content[1:]]\n",
    "test_matrix = []\n",
    "for i in content:\n",
    "    test_matrix.append(setOfWords2Vec(most_occurring_words_1000,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_matrix = np.array(test_matrix)\n",
    "test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa,pb,pe,pv = trainNB_extension(combined_train_matrix_extension[:,:-1],combined_train_matrix[:,-1],combined_train_matrix[:,:-1].astype(float))\n",
    "predict = generate_label_result(test_matrix,pa,pb,pe,pv,prior_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wxu520.csv', 'w', newline='') as csvfile:\n",
    "    writer  = csv.writer(csvfile)\n",
    "    writer.writerow([\"id\",\"class\"])\n",
    "    for i,label in enumerate(predict):\n",
    "        writer.writerow([i+1,label])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
